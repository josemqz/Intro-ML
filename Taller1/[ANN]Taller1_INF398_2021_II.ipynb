{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5rfZ_hYaQ1Z"
   },
   "source": [
    "<center><img src=\"http://www.exalumnos.usm.cl/wp-content/uploads/2015/06/Isotipo-Negro.gif\" title=\"Title text\" width=\"30%\" /></center>\n",
    "\n",
    "<hr style=\"height:2px;border:none\"/>\n",
    "<h1 align='center'> INF-398 Aprendizaje Automático </h1>\n",
    "\n",
    "<H3 align='center'> Tarea/Taller 1 </H3>\n",
    "<hr style=\"height:2px;border:none\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZ2x2ppMfRfu"
   },
   "source": [
    "# Temas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylwudRXniRBW"
   },
   "source": [
    "* Clasificadores Discriminativos Clásicos\n",
    "* Clasificadores Generativos Clásicos\n",
    "* Evaluación de Clasificadores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HvdvEYzKw3RL"
   },
   "source": [
    "# Reglas & Formalidades\n",
    "\n",
    "* Pueden trabajar en equipos de 2 a 3 personas. \n",
    "* Los equipos deben ser inscritos antes del 24 Septiembre.\n",
    "* Pueden reusar código visto en clases y/o recolectar código/ideas de otros sitios, mencionando al autor y entregando un link a la fuente. \n",
    "* Si resulta necesaria, la intervención de personas ajenas al grupo (e.g. experto) debe ser declarada y justificada.\n",
    "* Tener roles dentro del equipo está bien, pero al final del proceso, cada miembro debe entender y estar en condiciones de exponer todo el trabajo realizado. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RjXyIFcChiWu"
   },
   "source": [
    "## Entregables \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_BqodqziX7J"
   },
   "source": [
    "\n",
    "> * **Video:** Se debe preparar un video explicativo de **15 a 20 minutos** donde se describe la metodología utilizada, los resultados obtenidos y las conclusiones de la experiencia. \n",
    "\n",
    "> * **Código:** Se debe enviar un jupyter notebook con el código utilizado, de modo que sea posible **reproducir los resultados** presentados. Como alternativa, se puede entregar un link Github con el código fuente, incluyendo instrucciones precisas para ejecutar los experimentos. En cualquier caso (notebook o repo) el código debe estar ordenado y seccionado apropiadamente.\n",
    "\n",
    "> * **Conformidad Ética:** Se debe incluir una breve declaración ética en que se indique que el trabajo que se está enviando es un trabajo original, desarollado por los autores en conformidad con todas reglas antes mencionadas. Se debe también mencionar brevemente cuál fue la contribución de cada miembro del equipo. La declaración puede ser parte del notebook o estar en un archivo dentro del repo.\n",
    "\n",
    "> * **Defensa en vivo (video-conferencia):** El día de clases agendado para la discusión del taller, se seleccionarán aleatoriamente algunos equipos que presentarán oralmente su trabajo ante el curso. Los autores serán evaluados considerando la discusión y debate que generen entre sus pares. Los puntos obtenidos (positivos o negativos) se sumarán a la nota final de taller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CqQpB2uohlP5"
   },
   "source": [
    "## Fechas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8opRqEcUibfI"
   },
   "source": [
    "> * Defensas: 15 de Octubre, horario de clases.\n",
    "> * Fecha de entrega de vídeo: 16 de Octubre 23:59 Hrs. (1 días después de encuentro).\n",
    "> * Fecha de entrega de Jupyter (notebook): 15 de Octubre 08:00 (se pueden hacer actualizaciones hasta el 16 de Octubre 23:59 Hrs.). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8SqC2y5Vig2c"
   },
   "source": [
    "# Instrucciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-rhWdS_yyvG"
   },
   "source": [
    "La tarea se divide en dos secciones:\n",
    "\n",
    "\n",
    "\n",
    "> **1. Pregunta de Investigación**. Para esta parte, los autores deben elegir una hipótesis de investigación y diseñar un procedimiento experimental que permita reunir evidencia en contra o a favor de la misma. Es legítimo tomar una posición *a-priori* en base a lo que han aprendido en el curso, pero es importante analizar críticamente los resultados sin descartar hipótesis alternativas. \n",
    "\n",
    "> La metodología debe incluir al menos 3 datasets, de los cuales al menos 2 deben ser reales. Es deseable también que incluyan experimentos controlados sobre dataset sintéticos o semi-sintéticos no triviales diseñados por ustedes. Por ejemplo, para demostrar que un método logra ignorar variables irrelevantes se podrían crear variables \"fake\" manualmente. Experimentos de este último tipo que se basen en un dataset real contarán como realizados sobre \"dataset reales\".\n",
    "\n",
    "> Si no es relevante para la pregunta de investigación y en honor al tiempo, no es necesario llevar a cabo un análisis exploratorio detallado sobre cada dataset utilizado.\n",
    "\n",
    ">  **2. Desafío Kaggle**. Para esta parte, los autores enfrentarán un desafío en la plataforma Kaggle y serán calificados en base a su posición en el tablero de resultados y el puntaje obtenido.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_d7yADwXq0yb"
   },
   "source": [
    "<hr style=\"height:2px;border:none\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_uIArKuIj89d"
   },
   "source": [
    "# Parte 1. Pregunta de Investigación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Th2moALiMfxF"
   },
   "source": [
    "Reuna evidencia experimental para refutar o sostener una de las siguientes hipótesis u afirmaciones (máximo 2 equipos por hipótesis).\n",
    "\n",
    "Elegir tema acá **usando el nombre del equipo**:\n",
    "\n",
    "https://doodle.com/poll/qgw7h5xb72khqq9x?utm_source=poll&utm_medium=link\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fTkbRyusPMok"
   },
   "source": [
    "\n",
    "\n",
    "\n",
    "> **1. Clasificadores Discriminativos versus Generativos.** Con muy pocos ejemplos etiquetados, un clasificador generativo alcanza un mejor error de clasificación que un clasificador discriminativo. Sin embargo, a medida que aumenta el número de ejemplos, la situación se invierte.\n",
    "\n",
    "> **2. Perceptrón y Margen.** La cota teórica que relaciona el número de iteraciones del perceptrón con el margen no se verifica experimentalmente, sobre pasándose en la mayoría de los casos.\n",
    "\n",
    "> **3. Margen y Overfitting.** El error de predicción de un clasificador lineal no es directamente proporcional al margen obtenido, pero el grado de overfitting sí lo es.\n",
    "\n",
    "> **4. Regresión Logística Multi-class.**: En problemas multi-class, usar un regresor logístico con heurísticas como OVO permite obtener un mejor desempeño que la extensión nativa.\n",
    "\n",
    "> **5. Label Noise.**: Un clasificador de tipo generativo es extremadamente sensible a errores de etiquetación, es decir aún si un porcentaje pequeño (< 10%) de las etiquetas de entrenamiento está corrupta, su desempeño se deteriora significativamente (> 10% de acccuracy). \n",
    "\n",
    "> **6. Crowdsourcing.**: Al entrenar un clasificador logístico con múltiples anotaciones por dato (provistas por diferentes anotadores), el clasificador aprende a predecir la etiqueta mayoritaria.\n",
    "\n",
    "> **7. Regresión Logística con Pesos:** Modificar los pesos de cada clase en la función objetivo del clasificador logístico permite mejorar los resultados en problemas de clasificación desbalanceados. \n",
    "\n",
    "> **8. Texto & NLP.** En problemas de clasificación de texto, un modelo Bayesiano Ingenuo puede superar el desempeño de un clasificador discriminativo entrenado sobre una representación neuronal simple tipo AWE.\n",
    "\n",
    "> **9. Texto & NLP.** Al entrenar un clasificador logístico para texto, el uso de mecanismos para \"pesar\" los términos, como TF-IDF, no genera mejoras significativas ya que el clasificador \"aprende\" directamente estos pesos durante el entrenamiento. \n",
    "\n",
    "> **10. Entre LDA y QDA.** Un \"híbrido\" LDA/QDA supera tanto a QDA como a LDA.\n",
    "\n",
    "> Definición del Híbrido: denotemos por $\\hat{\\Sigma}_k$ las matrices de covarianza obtenidas por QDA y por $\\hat{\\Sigma}$ la (única) matriz de covarianza obtenida por LDA. El híbrido se define como un clasificador gausiano que usa $\\hat{\\Sigma}_{k} = (1-\\lambda) \\hat{\\Sigma}_k + \\lambda \\hat{\\Sigma}$ como matriz de covarianza para cada clase ($\\lambda$ debe ser seleccionado para cada problema).\n",
    "\n",
    "> **11. Instance Weighting**: Si se re-entrena un clasificador asignando mayor \"peso\" a los ejemplos que éste clasificó mal en un primer entrenamiento, observaremos una mejora en su desempeño final sobre el conjunto de pruebas.  \n",
    "\n",
    "> **12. Instance Weighting**: Si se re-entrena un clasificador asignando mayor \"peso\" a los ejemplos de entrenamiento más parecidos a los ejemplos de prueba (sólo x), observaremos una mejora en su desempeño final sobre ese último conjunto. \n",
    "\n",
    "> **13. Clases Desbalanceadas**: Un desbalance en la cantidad de ejemplos por clase afecta mucho más el desempeño de un clasificador discriminativo que el desempeño de un clasificador generativo, ya que en este último caso es posible ajustar manualmente los *a-priori* para corregir la situación.\n",
    "\n",
    "> **14. Datos Faltantes**: La imputación de atributos faltantes mediante criterios sencillos como la moda o la media deteriora significativamente el desempeño de un clasificador generativo, no así de un clasificador discriminativo. \n",
    "\n",
    "> **15. Métricas de Evaluación:** El área bajo la curva ROC es proporcional al área bajo la curva PR y por lo tanto un clasificador que supera a otro en términos de AUROC lo hace también en términos de AUPR.\n",
    "\n",
    "> **16. Métricas de Evaluación:** En problemas de clasificación con clases muy desbalanceadas, las métricas denominadas *Micro-average F-Score* y *AUPR* producen un ranking similar sobre un conjunto de clasificadores. \n",
    "\n",
    "> **17. Selección de Modelos:** Estimar el error de predicción de un clasificador usando un subconjunto de validación reservado desde dataset original produce resultados muy variables dependiendo del porcentaje seleccionado. Desafortundamente, lo mismo sucede con *K-fold crossvalidation* al considerar diferentes valores de K.\n",
    "\n",
    "> **18. Selección de Modelos:** El número de variables con que se entrena un modelo es inversamente proporcional al error de pruebas y directamente proporcional a la diferencia entre el error de validación y el error de entrenamiento. \n",
    "\n",
    "> **19. Selección de Modelos:** Seleccionar el valor de dos hiper-parámetros usando dos validaciones cruzadas independientes es tan efectivo como un utilizar un esquema anidado (nested CV).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mro0DvciPO8t"
   },
   "source": [
    "# Parte 2. Desafío\n",
    "\n",
    "> TO BE ANNOUNCED.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "[ANN]Taller1_INF398_2021-II.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
